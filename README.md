# Machine_Learning_com_Python


# Referências Bibliográficas



LARSON, Ron. **Estatística Aplicada**. São Paulo: Pearson Prentice Hall, 2010.

SUTSKEVER, I., MARTENS, J., HINTON, G., **“Generating Text With Recurrent
Neural Networks”**. In: 28 International Conference on Machine Learning, Bellevue, WA, USA, 2011.

Perceptrons. **An Introduction to Computational Geometry**. MARVIN MINSKY and SEYMOUR PAPERT. M.I.T. Press, Cambridge, Mass., 1969.

**Deep Learning Book**. Disponível em http://deeplearningbook.com.br/ . Acesso 02 de fevereiro de 2020.



# Links Úteis



https://medium.com/@msremigio/m%C3%A1quinas-de-vetores-de-suporte-svm-77bb114d02fc

https://towardsdatascience.com/understanding-support-vector-machine-part-1-lagrange-multipliers-5c24a52ffc5e

https://chrisalbon.com/code/machine_learning/support_vector_machines/svc_parameters_using_rbf_kernel/

https://anderfernandez.com/en/blog/code-logistic-regression-r-from-scratch/

https://www.researchgate.net/figure/A-k-nearest-neighbor-KNN-classifier-KNN-is-explained-as-follows_fig1_318096864

https://medium.com/machine-learning-beyond-deep-learning/%C3%A1rvores-de-decis%C3%A3o-3f52f6420b69

https://www.maxwell.vrac.puc-rio.br/7587/7587_4.PDF

https://www.tibco.com/reference-center/what-is-a-random-forest

https://www.datageeks.com.br/xgboost/

https://medium.com/@pushkarmandot/https-medium-com-pushkarmandot-what-is-lightgbm-how-to-implement-it-how-to-fine-tune-the-parameters-60347819b7fc

https://www.kaggle.com/samratp/boston-housing-prices-evaluation-validation

https://github.com/albertoivo/boston-house-price-mlnd/blob/master/boston_housing_PT.ipynb

https://scikit-learn.org/stable/modules/clustering.html

https://towardsdatascience.com/dbscan-algorithm-complete-guide-and-application-with-python-scikit-learn-d690cbae4c5d

https://www.kaggle.com/khotijahs1/k-means-clustering-of-iris-dataset

https://www.kaggle.com/shwetabh123/mall-customers

https://www.kaggle.com/aliderakhshesh/combination-of-pca-kmeans

https://acervolima.com/ml-explicacao-do-agrupamento-optics/

https://dcm.ffclrp.usp.br/~augusto/teaching/ami/AM-I-Regras-Associacao.pdf



# OTIMIZAÇÃO (AJUSTE) HIPERPARÂMETROS

https://aws.amazon.com/pt/what-is/hyperparameter-tuning/

https://medium.com/data-hackers/otimizando-os-hiperpar%C3%A2metros-621de5e9be37

# CURSO:

https://www.udemy.com/course/machine-learning-otimizacao-de-hiperparametros-com-python/?referralCode=849C2DC91D5D27790836

## Conteúdo do curso

29. Conceitos de Machine Learning

30. Etapas para criação dos algoritmos

31. Classificação

32. Conhecendo o Dataset

33. Exploração e análise dos dados – parte 1

34. Exploração e análise dos dados – parte 2

35. Exploração e análise dos dados – parte 3

36. Análise e tratamento dos dados – parte 1

37. Análise e tratamento dos dados – parte 2

38. Pré-processamento dos dados: variáveis categóricas

39. Pré-processamento: escalonamento e separação de variáveis

40. Pré-processamento: LabelEncoder e OnehotEncoder

41. Pré-processamento: Redução de dimensionalidade

42. Pré-processamento: Salvamento de variáveis

43. Separação entre treino e teste

44. Naive Bayes : Teoria

45. Naive Bayes no Python

46. Máquina de Vetor de Suporte (SVM): Teoria

47. Máquina de Vetor de Suporte no Python

48. Regressão Logística: Teoria

49. Regressão Logística no Python

50. KNN (Aprendizagem Baseada em Instâncias)

51. KNN no Python

52. Árvore de Decisão: Teoria

53. Árvore de Decisão no Python

54. Random Forest: teoria

55. Random Forest no Python

56. XGBoost: Teoria

57. XGBoost no Python

58. Light GBM: teoria

59. LGBM no Python

60. Catboost: Teoria

61. Catboost no Python

62. Salvando dados e simulando Deploy

63. Otimização de Hiperparâmetros

64. Otimização com Grid Search

65. Desafio 1

Tarefa 1: Postagem Desafio 1

66. Regressão

67. Conhecendo o Dataset

68. Exploração, Análise e Tratamento dos dados

69. Correlação e Regressão Linear Simples

70. Correlação Linear no Python

71. Regressão Linear no Python: parte 1

72. Regressão Linear no Python: parte 2

73. Avaliação da Regressão Linear Simples no StasModels

74. Regressão Linear Múltipla

75. Regressão Linear Múltipla no Python

76. Avaliação da Regressão Linear Múltipla no Statsmodels

77. Regressão Polinomial: teoria

78. Regressão Polinomial no Python

79. Regressão com Vetores de Suporte (SVR) no Python

80. Regressão com Árvore de decisão no Python

81. Regressão com Random Forest no Python

82. Regressão com XGBoost no Python

83. Regressão com Light GBM no Python

84. Regressão com CatBoost no Python

85. Salvando dados e simulando Deploy

86. Desafio 2

Tarefa 2: Postagem do Desafio 2

87. Neurônio Biológico e neurônio artificial

88. Perceptron

89. Funções de ativação

90. Aprendizagem nas redes neurais

91. Aprendizagem com descida do gradiente

92. Topologias das redes neurais artificiais

93. Algoritmo Backpropagation

94. Definição dos hiperparâmetros

95. Classificação com Redes Neurais Artificiais: pré-processamento

96. Classificação com Redes Neurais Artificiais: criação do algoritmo

97. Desafio 3

Tarefa 3: Postagem Desafio 3

98. Regressão Linear com Redes Neurais artificiais parte 1

99. Regressão Linear com Redes Neurais artificiais parte 2

100. Desafio 4

Tarefa 4: Postagem Desafio 4

101. Agrupamento

102. Conhecendo o Dataset

103. Exploração e tratamento dos dados

104. Pré-Processamento

105. K-Means (teoria)

106. K-means no Python: dois atributos

107. K-means no Python: todos atributos

108. K-means com PCA no Python
12m

109. Agrupamento Hierárquico

110. Hierárquico com PCA no Python

111. Hierárquico no Python: todos atributos

112. DBSCAN

113. DBSCAN com PCA no Python

114. DBSCAN no Python: todos atributos

115. MeanShift

116. MeanShift com PCA no Python

117. MeanShift no Python: todos atributos

118. K-Prototypes

119. k-Modes parte 1

120. K-modes parte 2

121. DESAFIO 5

Tarefa 5: Postagem Desafio 5

122. Regras de associação

123. ECLAT

124. APRIORI

125. Conhecendo o Dataset

126. Análise, tratamento e pré-processamento dos dados

127. Projeto Apriori

128. DESAFIO 6

Tarefa 6: Postagem Desafio 6

129. Conceitos da Aprendizagem por Reforço

130. Conhecendo o problema

131. Configurações do treinamento

132. Treinamento do Algoritmo

133. Avaliação do Algoritmo

134. Encerramento

135. Referências Bibliográficas
